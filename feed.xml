<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://adityash23.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://adityash23.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-21T23:46:16+00:00</updated><id>https://adityash23.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Exploring RAG and its components</title><link href="https://adityash23.github.io/blog/2025/text/" rel="alternate" type="text/html" title="Exploring RAG and its components"/><published>2025-07-07T14:24:00+00:00</published><updated>2025-07-07T14:24:00+00:00</updated><id>https://adityash23.github.io/blog/2025/text</id><content type="html" xml:base="https://adityash23.github.io/blog/2025/text/"><![CDATA[<h2 id="rag">RAG</h2> <p>As the usage of Large Language Models (LLMs) continues to grow, it becomes increasingly important how we retrieve and generate answers to complex queries. RAG (Retrieval-Augmented Generation) is a technology that combines document retrieval with model generation. It involves getting a user query, finding relevant documents, and using those documents as context to generate a more informed and accurate answer. Let’s explore RAG and its various components today!</p> <h2 id="query-translation--">Query translation -</h2> <h3 id="multi-query">Multi Query</h3> <ul> <li>From a single user input, multiple other similar queries (example - paraphrases) are generated and then all of these queries are used for search in the embedding space.</li> <li>It is particularly useful when the original user query might not be suited for the embedding space and will not find any similar documents at the end, the results from all these queries can be combined to give a single, comprehensive output.</li> </ul> <h3 id="rag-fusion">RAG Fusion</h3> <ul> <li>Same as Multi Query wherein each LLM generated query returns a list of documents as output.</li> <li>A ranking is done on all these documents and then the top X ranked documents are passed in to the LLM along with the original user input to give one final answer.</li> </ul> <h3 id="decomposition">Decomposition</h3> <ul> <li>The user input is broken down into sub-questions that will be solved individually.</li> <li>These are then answered sequentially and the result of past questions is used as context when answering future questions.</li> <li>The answer to the last question is then provided as the final output.</li> <li>An alternate version exists where the past answers are not used in future questions. Instead, all individual question-answer pairs are combined at the end to generate a final output to the original user question.</li> </ul> <h3 id="step-back">Step Back</h3> <ul> <li>Instead of answering the user question directly, a more abstract question is generated first and then its response is used to answer the user’s original question.</li> </ul> <h3 id="hyde">HyDe</h3> <ul> <li>To judge how “<em>close</em>” in meaning is one piece of text to another, Cosine Similarity is often used when both the pieces are represented as vectors in the same embedding space.</li> <li>To find a document containing a “<em>close enough</em>” answer to an input question, these 2 can’t be compared directly since they are 2 different entities (a document vs a question).</li> <li>To solve this, a hypothetical document is generated based on the input query and then this document is used to find the similar documents.</li> <li>From the existing set of actual documents with the expectation that this “<em>similar</em>” document will contain answer to the initial user query.</li> </ul> <h2 id="routing">Routing</h2> <p>Directing the user input to the appropriate data source. There are 2 ways to do so -</p> <h3 id="logical-routing">Logical routing</h3> <ul> <li>Uses a model to choose the data source which matches the domain of the query. <h3 id="semantic-routing--">Semantic routing -</h3> </li> <li>It uses internal, pre-defined prompts that are further used to compute a similarity search with the user input.</li> <li>The prompt with the highest similarity decides how the final answer will be generated (example - by specifying the domain of the query).</li> </ul> <h2 id="query-construction">Query construction</h2> <p>Involves going from unstructured, natural language input to structured query object that is tailored to the details provided about the database. Can be used to generate metadata search queries corresponding to an input text command.</p> <h2 id="indexing">Indexing</h2> <p>It involves generating numerical representation of text documents that can be efficiently searched for relevance to user inputs.</p> <h3 id="multi-representation-indexing">Multi representation Indexing</h3> <ul> <li>Each document in the database is converted into a “<em>summary</em>” that contains the document’s information and is optimized for retrieval. The documents and their summaries are linked.</li> <li>Any input question is now matched with a similar “<em>summary</em>” and the output is the document originally linked with this summary.</li> </ul> <h3 id="raptor-indexing">RAPTOR Indexing</h3> <ul> <li>All documents in the database are clustered together and summarized. This process is repeated until 1 summary remains or it has been repeated X times.</li> <li>The user input is now matched with these high-level summaries and thus allows for a lot more documents to be considered at once while generating an output.</li> <li>It shows improvement over simple document similarity search where only top Y documents can be considered at once.</li> </ul> <h3 id="colbert-indexing">ColBERT Indexing</h3> <ul> <li>So far, all techniques embed a whole document into a single vector which might lead to information loss. Instead, ColBERT breaks a document into tokens which are then embedded in the space.</li> <li>Any input query is also broken into tokens and embedded in the same space.</li> <li>For each query token, its similarity is computed against all document tokens and the one with max similarity is chosen.</li> <li>The <strong>Document Similarity Score</strong> is the sum of the similarities of each query token with its corresponding document token that gives max similarity.</li> </ul> <h2 id="crag--corrective-rag">CRAG : Corrective-RAG</h2> <p>This is one of the strategies of <strong>Active RAG</strong> wherein the user query is used to retrieve documents which are then ranked in terms of relevance to the query.</p> <ul> <li>Any relevant documents are passed as is to the context whereas irrelevant documents lead to a query transformation followed by an external information search for this transformed query (example - a web search).</li> <li>The result of this external search is added to the context along with other documents and now a final output is generated.</li> </ul>]]></content><author><name></name></author><category term="academic"/><category term="ml"/><category term="llm"/><summary type="html"><![CDATA[intro to RAG technology]]></summary></entry><entry><title type="html">Sustainable Machine Learning</title><link href="https://adityash23.github.io/blog/2025/text/" rel="alternate" type="text/html" title="Sustainable Machine Learning"/><published>2025-06-30T14:24:00+00:00</published><updated>2025-06-30T14:24:00+00:00</updated><id>https://adityash23.github.io/blog/2025/text</id><content type="html" xml:base="https://adityash23.github.io/blog/2025/text/"><![CDATA[<p>In the era of ChatGPT and other Generative AI models that allow you to get an answer to almost any question, let’s take a step back and rethink our model choice before just doing an <em>API call to yet another LLM</em>.</p>]]></content><author><name></name></author><category term="academic"/><category term="ml"/><summary type="html"><![CDATA[intro to energy consumption of ML models]]></summary></entry><entry><title type="html">First Post</title><link href="https://adityash23.github.io/blog/2025/text/" rel="alternate" type="text/html" title="First Post"/><published>2025-06-23T14:24:00+00:00</published><updated>2025-06-23T14:24:00+00:00</updated><id>https://adityash23.github.io/blog/2025/text</id><content type="html" xml:base="https://adityash23.github.io/blog/2025/text/"><![CDATA[<p>First few steps after you make your al-folio website -</p> <ul> <li>set up your personal info in _config.yml</li> <li>update socials in socials.yml</li> <li>clean up nav bar and decide which menu items to display</li> <li>set up which github user and repos to show on repositories page</li> <li>clean up readme_preview images</li> <li>clean up audio, video in assets</li> <li>modify tags that show up on the blog page using display_tags in config.yml</li> </ul>]]></content><author><name></name></author><category term="general"/><category term="general"/><summary type="html"><![CDATA[a brand new post]]></summary></entry></feed>